# Plan de Pruebas de Carga ‚Äì ANB_App_API

**Proyecto:** Asociaci√≥n Nacional de Baloncesto (ANB)  
**Herramienta:** Apache JMeter  
---

## üéØ 1. Objetivos

- Dise√±ar y documentar escenarios de **carga y estr√©s** para rutas cr√≠ticas (web y batch).  
- Medir **throughput**, **tiempos de respuesta** y **utilizaci√≥n de recursos**.  
- Dejar configurada la infraestructura para las entregas futuras (2 a 5).

---

## ‚öôÔ∏è 2. Alcance funcional

Endpoints incluidos:

| Grupo | Endpoint | M√©todo | Descripci√≥n |
|--------|-----------|---------|-------------|
| Autenticaci√≥n | `/api/auth/signup` | POST | Registro de jugadores |
| Autenticaci√≥n | `/api/auth/login` | POST | Inicio de sesi√≥n |
| Videos | `/api/videos/upload` | POST | Subida de video |
| Videos | `/api/videos` | GET | Listado de videos del usuario |
| Videos | `/api/videos/{id}` | GET/DELETE | Detalle o eliminaci√≥n |
| P√∫blico | `/api/public/videos` | GET | Lista de videos p√∫blicos |
| P√∫blico | `/api/public/videos/{id}/vote` | POST | Votar un video |
| Ranking | `/api/public/rankings` | GET | Ranking general |

---

## üß† 3. Entorno de pruebas

**Infraestructura de prueba:**
- Instancia EC2: `m5.large` (2 vCPU, 8 GB RAM)
- Regi√≥n: `us-east-1`
- Herramientas: Apache JMeter (CLI y GUI), Docker opcional
- Monitoreo: AWS CloudWatch (CPU, RAM, red), opcional InfluxDB + Grafana

---

## üåê 3.1. Topolog√≠a del entorno de prueba

La **topolog√≠a** describe c√≥mo se conectan los distintos componentes durante la ejecuci√≥n de las pruebas de carga.

- **Cliente de carga:** instancia EC2 dedicada que ejecuta JMeter.  
- **Balanceador / API Gateway:** punto de entrada de la aplicaci√≥n (recibe las peticiones).  
- **Backend:** servicios que procesan la l√≥gica del API (ECS, Lambda o EC2).  
- **Almacenamiento:** buckets S3 y base de datos relacional o NoSQL.  
- **Monitoreo:** m√©tricas recolectadas desde CloudWatch y, opcionalmente, Grafana.

**Diagrama de topolog√≠a:**

```mermaid
graph LR
  A[JMeter EC2<br/>Generador de carga] --> B[API Gateway / ALB<br/>Balanceador de entrada]
  B --> C[Backend ECS / Lambda / EC2<br/>Servicios de aplicaci√≥n]
  C --> D[(S3 / Base de datos)]
  A -.-> E[CloudWatch / Grafana<br/>Monitoreo de rendimiento]
```
## üìä 4. M√©tricas y criterios de aceptaci√≥n

Las pruebas de carga evaluar√°n el desempe√±o del sistema en funci√≥n de **tres m√©tricas principales**:  
**throughput (capacidad de procesamiento)**, **tiempo de respuesta** y **utilizaci√≥n de recursos**, tal como lo indica la gu√≠a institucional.

---

### üîπ M√©tricas principales

| M√©trica | Descripci√≥n | Fuente / Herramienta |
|----------|--------------|----------------------|
| **Throughput** | N√∫mero de peticiones procesadas por segundo. Mide la capacidad de procesamiento del sistema. | JMeter Summary Report |
| **Tiempo de respuesta (avg, P95, P99)** | Tiempo transcurrido desde que el usuario env√≠a una solicitud hasta recibir la respuesta completa. | JMeter HTML Report |
| **Tasa de errores (%)** | Porcentaje de respuestas HTTP con errores (4xx, 5xx) respecto al total de peticiones. | JMeter Summary Report |
| **Utilizaci√≥n de recursos** | Porcentaje de uso de CPU, memoria y red del backend durante la carga. | AWS CloudWatch o Grafana |

---

### üéØ Criterios de aceptaci√≥n

Estos criterios definen los umbrales de desempe√±o esperados para considerar las pruebas exitosas:

| Escenario / Endpoint | Criterio P95 | Tasa de errores | Restricciones de recursos |
|----------------------|-------------:|----------------:|---------------------------|
| **Login** | < 2 segundos | < 1% | CPU < 70%, RAM < 75% |
| **Upload (video peque√±o)** | < 4 segundos | < 2% | CPU < 80%, RAM < 80% |
| **Listados / Detalle de videos** | < 2.5 segundos | < 1% | CPU < 70%, RAM < 75% |
| **Batch / Procesamiento as√≠ncrono** | < 6 segundos | < 3% | CPU < 85%, RAM < 85% |

---

### üìà M√©tricas adicionales recomendadas

- **Throughput sostenido m√≠nimo:** ‚â• 50 solicitudes por segundo.  
- **Disponibilidad:** ‚â• 99% durante la duraci√≥n de la prueba.  
- **Picos de carga tolerables:** El sistema no debe degradarse dr√°sticamente cuando la concurrencia se duplica.  
- **Escalabilidad:** La relaci√≥n entre usuarios concurrentes y throughput debe mantenerse casi lineal hasta la carga nominal.

---

## 5. Escenarios de prueba

Los escenarios de prueba fueron dise√±ados para simular tanto el uso normal de la aplicaci√≥n como situaciones de alta concurrencia que permitan evaluar el rendimiento, estabilidad y escalabilidad del sistema.

---

### üß© Escenario 1 ‚Äî Ruta cr√≠tica web: Registro, Login y Carga de Video

**Objetivo:** Validar el flujo principal del jugador, desde el registro hasta la visualizaci√≥n de sus videos cargados.

**Flujo:**

1. `POST /api/auth/signup` ‚Üí Registro de nuevo jugador  
2. `POST /api/auth/login` ‚Üí Autenticaci√≥n y obtenci√≥n de token JWT  
3. `POST /api/videos/upload` ‚Üí Subida de video  
4. `GET /api/videos` ‚Üí Consulta de videos subidos por el usuario  

**Configuraci√≥n de carga:**

| Tipo de prueba | Usuarios concurrentes | Ramp-up | Duraci√≥n total |
|----------------|----------------------:|--------:|----------------:|
| **Smoke test** | 10 | 30 s | 2 min |
| **Carga nominal** | 100 | 120 s | 10 min |
| **Estr√©s** | 500 | 300 s | 15 min |

**Validaciones esperadas (Assertions):**
- `signup`: c√≥digo **201 Created**  
- `login`: c√≥digo **200 OK** y token v√°lido  
- `upload`: c√≥digo **201 Created** con identificador de tarea (`task_id`)  
- `list`: c√≥digo **200 OK**

**Prop√≥sito t√©cnico:**  
Comprobar el comportamiento de los endpoints m√°s usados por el usuario final en condiciones normales y bajo carga alta.  

---

### üß© Escenario 2 ‚Äî Procesamiento batch: Subida, Lectura y Voto

**Objetivo:** Evaluar la capacidad del sistema durante operaciones as√≠ncronas o de larga duraci√≥n, donde m√∫ltiples usuarios realizan tareas simult√°neas de carga y votaci√≥n.

**Flujo:**

1. `POST /api/videos/upload` ‚Üí Subida de video a cola de procesamiento  
2. Backend procesa el video (asincron√≠a)  
3. `GET /api/public/videos` ‚Üí Lectura de videos p√∫blicos procesados  
4. `POST /api/public/videos/{id}/vote` ‚Üí Registro de voto del jugador  

**Configuraci√≥n de carga:**

| Tipo de prueba | Usuarios concurrentes | Ramp-up | Duraci√≥n total | Mix de acciones |
|----------------|----------------------:|--------:|----------------:|----------------:|
| **Carga nominal** | 50 | 180 s | 15 min | 30% upload / 50% list / 20% vote |
| **Estr√©s alto** | 200 | 300 s | 20 min | 40% upload / 40% list / 20% vote |

**Validaciones esperadas (Assertions):**
- `upload`: c√≥digo **201 Created** (en cola)  
- `list`: c√≥digo **200 OK**  
- `vote`: c√≥digo **200 OK** o **400 Bad Request** (voto duplicado)  

**Prop√≥sito t√©cnico:**  
Medir el rendimiento durante el procesamiento intensivo de videos y la interacci√≥n simult√°nea de usuarios en endpoints p√∫blicos.

---

### üß© Escenario 3 ‚Äî Prueba de resistencia (Endurance)

**Objetivo:** Evaluar la estabilidad del sistema durante una carga prolongada.

**Flujo:**
- `POST /api/auth/login`  
- `GET /api/public/videos`  
- `POST /api/public/videos/{id}/vote`

**Configuraci√≥n de carga:**

| Tipo de prueba | Usuarios concurrentes | Ramp-up | Duraci√≥n total |
|----------------|----------------------:|--------:|----------------:|
| **Endurance** | 100 | 120 s | 60 min |

**Validaciones esperadas:**
- Tiempo de respuesta estable durante toda la prueba  
- Sin incremento progresivo en tasa de errores  
- Recursos del servidor dentro de l√≠mites aceptables  

---

### üéØ Criterios de √©xito de los escenarios

| Tipo de escenario | Objetivo de validaci√≥n | Criterio de √©xito |
|-------------------|------------------------|-------------------|
| **Ruta cr√≠tica web** | Validar tiempos de respuesta en operaciones principales | P95 < 4 s y errores < 2% |
| **Batch** | Medir rendimiento del procesamiento as√≠ncrono | P95 < 6 s y CPU < 85% |
| **Endurance** | Evaluar estabilidad a largo plazo | Variaci√≥n de throughput < ¬±10% |

---

### ‚öôÔ∏è Configuraci√≥n com√∫n en JMeter

- **Thread Group** con ramp-up variable.  
- **HTTP Header Manager:** `Content-Type: application/json`.  
- **Timers:** `Uniform Random Timer (500‚Äì1500 ms)`.  
- **JSON Extractors:** para variables din√°micas (`token`, `video_id`).  
- **Assertions:** validar c√≥digos HTTP esperados.  
- **Simple Data Writer:** para guardar resultados (`.jtl`).  

**Variables de entorno parametrizadas:**
```
BASE_URL_AUTH
BASE_URL_VIDEO
BASE_URL_PUBLIC
USERS=${__P(users,100)}
RAMP=${__P(ramp,120)}
DURATION=${__P(duration,600)}
```

---
## 6. Datos de prueba

Los datos de prueba se definen para garantizar que las solicitudes enviadas por JMeter simulen correctamente las acciones reales de los usuarios, evitando errores por duplicados o inconsistencias en el backend.

---

### üßæ Fuentes de datos

Los datos provienen de tres tipos de or√≠genes:

1. **Variables din√°micas generadas por JMeter**  
   - Se usan funciones internas de JMeter (`__time`, `__RandomString`, `__UUID`) para crear identificadores √∫nicos por cada ejecuci√≥n.  
   - Ejemplo: `user_${__time(YMMddHHmmss)}` ‚Üí genera un correo distinto en cada iteraci√≥n.  

2. **Archivos CSV externos**  
   - Se cargan mediante el componente **CSV Data Set Config** de JMeter.  
   - Contienen informaci√≥n base como nombres, contrase√±as o identificadores de video ya existentes.  
   - Ejemplo: `usuarios_test.csv`, `videos_seed.csv`.

3. **Valores obtenidos en tiempo de ejecuci√≥n**  
   - A trav√©s de **JSON Extractors** se capturan datos de la respuesta de un endpoint (por ejemplo, `token`, `video_id`) para ser usados en peticiones posteriores.

---

### üìÇ Estructura de datos usada en las pruebas

| Tipo de dato | Ejemplo | Uso principal |
|---------------|----------|----------------|
| **Correo electr√≥nico** | `user_${timestamp}@test.com` | Registro y login |
| **Contrase√±a** | `password_${timestamp}` | Autenticaci√≥n de usuario |
| **Token JWT** | `token_player_test` | Autorizaci√≥n en endpoints protegidos |
| **ID de video** | `video_id_test` | Lectura y voto de videos |
| **Archivos multimedia** | `mp4_small_file`, `mp4_large_file`, `mp4_large_duration_file`, `pdf_test_file` | Pruebas de carga en `/upload` |
| **Texto de descripci√≥n** | `"Video de prueba ${__RandomString(5,abcdefghijklmnopqrstuvwxyz)}"` | Campos opcionales en subida de video |

---

### ‚öôÔ∏è Configuraci√≥n de variables en JMeter

Las variables se declaran y utilizan dentro de los **HTTP Request** y **Pre/Post-Processors**.

Ejemplo de uso dentro de un cuerpo JSON:
```json
{
  "email": "${email}",
  "password": "${password}",
  "videoTitle": "${video_title}",
  "description": "${description}"
}
```

**Variables definidas globalmente:**
```
BASE_URL_AUTH
BASE_URL_VIDEO
BASE_URL_PUBLIC
USERS=${__P(users,100)}
RAMP=${__P(ramp,120)}
DURATION=${__P(duration,600)}
```
---
## 7. Configuraci√≥n JMeter

Esta secci√≥n describe la estructura, componentes y par√°metros utilizados en Apache JMeter para ejecutar las pruebas de carga sobre los endpoints definidos de la API.

---

### ‚öôÔ∏è Estructura general del plan de pruebas

El plan de pruebas se compone de los siguientes elementos principales:

1. **Test Plan**  
   - Contiene todas las configuraciones globales del proyecto (variables, propiedades, archivos CSV).  
   - Incluye las rutas base y credenciales si son necesarias.

2. **Thread Groups (Grupos de hilos)**  
   - Simulan usuarios concurrentes enviando solicitudes HTTP.  
   - Cada grupo corresponde a un escenario (por ejemplo, *Ruta cr√≠tica* o *Batch*).  
   - Par√°metros b√°sicos:
     - **Usuarios (threads):** n√∫mero de usuarios virtuales concurrentes.  
     - **Ramp-up:** tiempo en segundos para alcanzar la cantidad total de usuarios.  
     - **Loop count:** n√∫mero de veces que se repite el escenario.

3. **HTTP Request Samplers**  
   - Representan las solicitudes a los endpoints de la API.  
   - Cada sampler define el m√©todo (GET, POST, DELETE), la URL y el cuerpo de la petici√≥n.  

4. **Config Elements**
   - **HTTP Header Manager:** define los encabezados comunes como `Content-Type: application/json`.  
   - **CSV Data Set Config:** carga datos desde archivos CSV si se requieren.  
   - **User Defined Variables:** contiene variables globales reutilizables (por ejemplo, `BASE_URL`, `TOKEN`).

5. **Timers**
   - Controlan los intervalos entre peticiones para simular pausas humanas.  
   - Se recomienda el uso de **Uniform Random Timer (500‚Äì1500 ms)**.

6. **Post-Processors**
   - **JSON Extractor:** extrae valores como `token` o `video_id` desde las respuestas JSON.  
   - **Regular Expression Extractor:** alternativo para capturar datos espec√≠ficos.

7. **Assertions**
   - Verifican que las respuestas cumplan condiciones esperadas (por ejemplo, c√≥digos HTTP 200 o 201).

8. **Listeners**
   - Registran los resultados de la prueba.  
   - Se recomienda usar:
     - **View Results Tree** (solo en modo debug).  
     - **Simple Data Writer** para guardar resultados en `.jtl`.  
     - **Summary Report** y **Aggregate Report** para an√°lisis de m√©tricas.  

---

### üß© Variables de entorno y propiedades del plan

Las variables globales del proyecto se definen en el plan de pruebas o se pasan como par√°metros al ejecutar JMeter desde la l√≠nea de comandos.

**Variables globales:**
```
BASE_URL_AUTH
BASE_URL_VIDEO
BASE_URL_PUBLIC
USERS=${__P(users,100)}
RAMP=${__P(ramp,120)}
DURATION=${__P(duration,600)}
```

Estas variables permiten reutilizar la misma configuraci√≥n para diferentes entornos (QA, staging, producci√≥n) y ajustar la carga sin modificar los archivos `.jmx`.

---

### üß† Configuraci√≥n de cabeceras HTTP

Cada solicitud debe incluir los encabezados correctos para autenticaci√≥n y tipo de contenido:

| Encabezado | Valor |
|-------------|--------|
| `Content-Type` | `application/json` |
| `Authorization` | `Bearer ${token}` (solo en endpoints protegidos) |

---

### üß™ Ejemplo de ejecuci√≥n en CLI

Para ejecutar los escenarios desde consola sin interfaz gr√°fica:

```bash
# Escenario 1 - Ruta cr√≠tica web
jmeter -n -t escenarios/escenario_1_ruta_critica.jmx \
  -Jusers=100 -Jramp=120 -Jduration=600 \
  -l resultados/web_nominal.jtl -e -o resultados/report_web

# Escenario 2 - Procesamiento batch
jmeter -n -t escenarios/escenario_2_batch.jmx \
  -Jusers=200 -Jramp=300 -Jduration=1200 \
  -l resultados/batch_stress.jtl -e -o resultados/report_batch
```

**Ejecuci√≥n en Docker:**

```bash
docker run --rm -v "$PWD":/test -w /test justb4/jmeter:latest \
  -n -t escenarios/escenario_1_ruta_critica.jmx \
  -Jusers=100 -Jramp=120 -Jduration=600 \
  -l resultados/web_nominal.jtl -e -o resultados/report_web
```

---

### üìä Resultados esperados de salida

Al finalizar cada ejecuci√≥n, JMeter generar√°:

- Un archivo `.jtl` con el log detallado de cada petici√≥n.  
- Un reporte HTML con m√©tricas de:
  - Throughput (requests/sec)  
  - Tiempo medio, P95 y P99 de respuesta  
  - Error rate (%)  
  - Distribuci√≥n de latencia  

Los resultados pueden abrirse en cualquier navegador desde la carpeta definida en `-o resultados/report_*`.

---
## 8. Plan de ejecuci√≥n

El plan de ejecuci√≥n define el orden, la frecuencia y las condiciones bajo las cuales se llevar√°n a cabo las pruebas de carga y rendimiento.  
Su objetivo es garantizar que los resultados sean reproducibles y permitan identificar claramente los l√≠mites de desempe√±o del sistema.

---

### üóìÔ∏è Fases del plan de ejecuci√≥n

| Fase | Objetivo | Descripci√≥n |
|------|-----------|--------------|
| **1. Smoke Test** | Verificar conectividad y estabilidad b√°sica | Ejecutar una prueba corta (5‚Äì10 usuarios) para confirmar que los endpoints responden correctamente. |
| **2. Prueba de carga nominal** | Medir desempe√±o bajo condiciones normales | Simular el comportamiento de usuarios concurrentes promedio (100‚Äì200 usuarios). |
| **3. Prueba de estr√©s** | Identificar el punto de degradaci√≥n | Aumentar progresivamente la cantidad de usuarios hasta que el sistema comience a mostrar errores o latencias elevadas. |
| **4. Prueba de resistencia (Endurance)** | Evaluar estabilidad a largo plazo | Mantener una carga constante durante 30‚Äì60 minutos para observar fugas de memoria o ca√≠das de rendimiento. |
| **5. An√°lisis y conclusiones** | Consolidar resultados y generar reportes | Revisar m√©tricas (P95, throughput, errores, CPU/RAM) y elaborar el informe final. |

---

### üß© Flujo general de ejecuci√≥n

1. **Preparaci√≥n del entorno**
   - Verificar conectividad con el servidor AWS.  
   - Asegurar que los endpoints est√©n desplegados en el entorno correcto.  
   - Validar las variables globales en JMeter (`BASE_URL_*`, `USERS`, `RAMP`, `DURATION`).

2. **Ejecuci√≥n de pruebas**
   - Iniciar con el escenario de **Smoke Test**.  
   - Continuar con **Ruta Cr√≠tica (Carga Nominal)**.  
   - Aumentar gradualmente hasta el escenario de **Estr√©s**.  
   - Finalizar con la **prueba de resistencia**.  

3. **Recolecci√≥n de resultados**
   - Guardar los archivos `.jtl` de cada ejecuci√≥n.  
   - Generar reportes HTML (`-e -o resultados/...`).  
   - Exportar m√©tricas clave a CSV o Google Sheets si se requiere.  

4. **Monitoreo paralelo**
   - Activar **AWS CloudWatch** o **Grafana** para observar CPU, memoria, I/O y red durante las pruebas.  
   - Correlacionar los picos de latencia con el uso de recursos.

---

### üìä M√©tricas que se registrar√°n

| M√©trica | Fuente | Frecuencia de captura | Uso |
|----------|---------|----------------------:|-----|
| **Throughput (req/s)** | JMeter Summary Report | Cada iteraci√≥n | Evaluar capacidad del sistema |
| **Tiempo de respuesta (avg/P95/P99)** | JMeter | Cada request | Analizar latencia bajo carga |
| **Errores (4xx/5xx)** | JMeter / API | Cada request | Medir estabilidad del backend |
| **Uso de CPU (%)** | CloudWatch | Cada 10 s | Correlacionar carga con recursos |
| **Uso de RAM (%)** | CloudWatch | Cada 10 s | Identificar posibles fugas o saturaci√≥n |
| **Tasa de disponibilidad (%)** | JMeter | Al finalizar cada escenario | Validar resiliencia del sistema |

---


### ‚öôÔ∏è Criterios de repetici√≥n

- Si la tasa de errores supera el **5 %**, repetir el escenario con menos usuarios para confirmar si el error es consistente.  
- Si los tiempos P95 superan el **doble del umbral esperado**, verificar logs del backend y repetir la prueba.  
- Cada escenario se debe ejecutar **m√≠nimo dos veces** para confirmar la reproducibilidad de los resultados.

---

